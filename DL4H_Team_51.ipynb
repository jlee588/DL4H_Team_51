{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team github link: https://github.com/jlee588/DL4H_Team_51\n",
        "\n",
        "Presentation link: https://www.youtube.com/watch?v=E-S_mWwDcrM"
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bring in necessary files from github\n"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jlee588/DL4H_Team_51"
      ],
      "metadata": {
        "id": "Cduo2t8Oh_Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/markdown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * This paper aims to solve the problem of predicting drug to drug interaction events for pharmaceutical research.\n",
        "  * The importance of solving this problem is that most research has been done on whether two drugs interact with each other but not many have analyzed the consequential effects of when two drugs interact with each other. The paper also stated that \"67% of elderly Americans took five or more medications in 2010-2011.\" This is important because with so many people taking multiple medications, it is imperative that we ensure those medications, if taken together, will still be safe to take.\n",
        "  * This problem can be difficult because the most relevant and diverse features must be chosen to use in a deep neural network in order to retrieve the most informative results from the model.It is important to choose the right amount of diverse features that will all individually contribute to the final results because that is how we will get the most informative model.\n",
        "  * The current state of the art methods in DDI predictions include DeepDDI, random forest, logistic regression, and K-nearest neighbor. From the paper, we will see that these methods have accuracy results of 0.8371,0.7775,0.7214, and 0.7920 respectively. They also have an area under the precision-recall curve of 0.8899,0.8349,0.7716, and 0.8400 respectively.\n",
        "\n",
        "*   Paper explanation\n",
        "  * The paper proposes a DNN-based architecture that combines a variety of drug features with deep learning to predict the DDI events. The features used in this model will be chemical substructures, targets, enzymes and pathways.\n",
        "  * The innovations of this method include using semantic analysis on the descriptions of known DDIs from the DrugBank dataset to construct an interaction event dataset and then using diverse features from drugs to predict DDI events.\n",
        "  * According to the paper, the results of the DDIMDL model outperformed all 4 other state of the art algorithms with an accuracy of 0.8852 and an area under the precision-recall curve of 0.9208.\n",
        "  * The contributions to the research regime from this paper include a multimodal deep learning framework that outperforms other state of the art methods in regards to DDI event predictions. This can be very helpful for the pharmaceutical industry as there is now an even higher chance of drug to drug interactions being detected and the adverse effects of these drugs being taken together can be avoided."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: A DNN-based architecture, specifically named DDIMDL, is superior to other state of the art algorithms in regards to predicting DDIs. To test this, our model will compare models that use the DDIMDL algorithm to predict DDIs with other state of the art algorithms such as random forest, logistic regression, gradient boosting decision tree, and K-nearest neighbor while all other aspects remain the same.\n",
        "Our ablations planned are to remove one or two layers from the model and compare those results from the original model.\n",
        "\n",
        "To reproduce the paper's results, we plan to modernize the code to work around deprecated functions and this will be shown in the commented out code throughout the file that will be replaced with the modern code below it.\n",
        "\n",
        "For example, np.mat() was deprecated, so we used np.array() instead like below\n",
        "    #     matrix = np.mat(matrix)\n",
        "    #     matrix = np.array(matrix)\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "*   Environment: Google Colab Notebook with CPU runtime\n",
        "*   Python: Python 3.10.2\n",
        "*   Packages: Needed packages imported below\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import csv\n",
        "import sqlite3\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression  # Corrected import\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (drug, event, interaction) tables from event.db, pre-procssed data (extraction) table from event.db, descriptive statistics (size of each table and basic info), and data processing (feature engineering).\n",
        "  * Source of the data: pre-processed data using Drugbank (https://go.drugbank.com/) is available from the public github of the paper (https://github.com/YifanDengWHU/DDIMDL) as a database file (event.db). A separate NLP script (NLPProcess.py available in github) was used to extract drug interactions to form extractions table in event.db\n",
        "  * Statistics: Functions to print out basic descriptive statistics are defined below.\n",
        "  * Data process: Data processing functions to split the data and convert them to feature vectors are defined below.\n",
        "  * Illustration: Functions for printing results of the model are defined below."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "All raw data needed are in event.db file\n",
        "event.db is also available in public github: https://github.com/jlee588/DL4H_Team_51\n",
        "Each dataframe is constructed by connecting to the event.db file via a sql query as shown below\n",
        "'''\n",
        "\n",
        "raw_data_dir = '/content/DL4H_Team_51/event.db'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  conn = sqlite3.connect(raw_data_dir)\n",
        "  df_drug = pd.read_sql('select * from drug;', conn)\n",
        "  df_event = pd.read_sql('select * from event_number;', conn)\n",
        "  df_interaction = pd.read_sql('select * from event;', conn)\n",
        "  return df_drug, df_event, df_interaction\n",
        "\n",
        "df_drug, df_event, df_interaction  = load_raw_data(raw_data_dir)\n",
        "conn = sqlite3.connect(raw_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate statistics\n",
        "'''\n",
        "1.drug contains 572 kinds of drugs and their features.\n",
        "2.event contains the 37264 DDIs between the 572 kinds of drugs.\n",
        "3.event_number lists the kinds of DDI events and their occurence frequency.\n",
        "'''\n",
        "def calculate_stats(dataframes):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  for df_name, df in dataframes.items():\n",
        "        print(f\"Statistics for {df_name}:\")\n",
        "        print(\"Shape:\", df.shape)\n",
        "        print(\"Info:\")\n",
        "        df.info()\n",
        "        print(\"=\"*40)\n",
        "\n",
        "calculate_stats({\n",
        "    'Drug': df_drug,\n",
        "    'Event Number': df_event,\n",
        "    'Interaction': df_interaction\n",
        "})"
      ],
      "metadata": {
        "id": "HwfhHOxkB4UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function works with raw data (df_drug) table and feature_vector function\n",
        "to create a feature vector with new labels\n",
        "It creates and returns:\n",
        "new_feature: array with combined feature vectors for drug pairs\n",
        "new_label: array of labels for each drug pair interaction\n",
        "event_num: number of events to be used with other functions\n",
        "\n",
        "'''\n",
        "def prepare(df_drug, feature_list, vector_size,mechanism,action,drugA,drugB):\n",
        "    d_label = {}\n",
        "    d_feature = {}\n",
        "    # Transfrom the interaction event to number\n",
        "    # Splice the features\n",
        "    d_event=[]\n",
        "    for i in range(len(mechanism)):\n",
        "        d_event.append(mechanism[i]+\" \"+action[i])\n",
        "    label_value = 0\n",
        "    count={}\n",
        "    for i in d_event:\n",
        "        if i in count:\n",
        "            count[i]+=1\n",
        "        else:\n",
        "            count[i]=1\n",
        "    list1 = sorted(count.items(), key=lambda x: x[1],reverse=True)\n",
        "    for i in range(len(list1)):\n",
        "        d_label[list1[i][0]]=i\n",
        "    vector = np.zeros((len(np.array(df_drug['name']).tolist()), 0), dtype=float)\n",
        "    for i in feature_list:\n",
        "        vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))\n",
        "    # Transfrom the drug ID to feature vector\n",
        "    for i in range(len(np.array(df_drug['name']).tolist())):\n",
        "        d_feature[np.array(df_drug['name']).tolist()[i]] = vector[i]\n",
        "    # Use the dictionary to obtain feature vector and label\n",
        "    new_feature = []\n",
        "    new_label = []\n",
        "    name_to_id = {}\n",
        "    for i in range(len(d_event)):\n",
        "        new_feature.append(np.hstack((d_feature[drugA[i]], d_feature[drugB[i]])))\n",
        "        new_label.append(d_label[d_event[i]])\n",
        "    new_feature = np.array(new_feature)\n",
        "    new_label = np.array(new_label)\n",
        "    return (new_feature, new_label, event_num)"
      ],
      "metadata": {
        "id": "Rqc_55vsaE0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function first defines a function to create a Jaccard similarity matrix to\n",
        "be used to quantify the similarity between every pair of drugs based on shared\n",
        "features.\n",
        "It then extracts and compiles a list of unique features from df_drug table\n",
        "Jaccard function is used to generate similarity matrices\n",
        "PCA is applied to reduce the dimensionality to vector_size\n",
        "'''\n",
        "def feature_vector(feature_name, df, vector_size):\n",
        "    # df are the 572 kinds of drugs\n",
        "    # Jaccard Similarity\n",
        "    # np.matrix outdated\n",
        "    # def Jaccard(matrix):\n",
        "    #     matrix = np.mat(matrix)\n",
        "    #     numerator = matrix * matrix.T\n",
        "    #     denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n",
        "    #     return numerator / denominator\n",
        "    def Jaccard(matrix):\n",
        "        matrix = np.array(matrix)\n",
        "        numerator = matrix.dot(matrix.T)\n",
        "        denominator = (np.ones(matrix.shape) @ matrix.T) + (matrix @ np.ones(matrix.T.shape)) - (matrix.dot(matrix.T))\n",
        "        denominator[denominator == 0] = 1\n",
        "        return numerator / denominator\n",
        "\n",
        "    all_feature = []\n",
        "    drug_list = np.array(df[feature_name]).tolist()\n",
        "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
        "    for i in drug_list:\n",
        "        for each_feature in i.split('|'):\n",
        "            if each_feature not in all_feature:\n",
        "                all_feature.append(each_feature)  # obtain all the features\n",
        "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
        "    df_feature = DataFrame(feature_matrix, columns=all_feature)  # Consrtuct feature matrices with key of dataframe\n",
        "    for i in range(len(drug_list)):\n",
        "        for each_feature in df[feature_name].iloc[i].split('|'):\n",
        "            df_feature[each_feature].iloc[i] = 1\n",
        "    sim_matrix = Jaccard(np.array(df_feature))\n",
        "\n",
        "    sim_matrix1 = np.array(sim_matrix)\n",
        "    count = 0\n",
        "    pca = PCA(n_components=vector_size)  # PCA dimension\n",
        "    pca.fit(sim_matrix)\n",
        "    sim_matrix = pca.transform(sim_matrix)\n",
        "    return sim_matrix"
      ],
      "metadata": {
        "id": "gIvGB8nWahwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function is used to assign labels and split the dataset for KFold cross-\n",
        "validation.\n",
        "It assigns a fold number 'k_num' to each test index in the index_all_classes array\n",
        "'''\n",
        "def get_index(label_matrix, event_num, seed, CV):\n",
        "    index_all_class = np.zeros(len(label_matrix))\n",
        "    for j in range(event_num):\n",
        "        index = np.where(label_matrix == j)\n",
        "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
        "        k_num = 0\n",
        "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
        "            index_all_class[index[0][test_index]] = k_num\n",
        "            k_num += 1\n",
        "\n",
        "    return index_all_class"
      ],
      "metadata": {
        "id": "1MQqnbm5FHyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The main model (DDIMDL) is defined below.\n",
        "All the layers needed (Dense, Dropout, BatchNormalization, Activation) are defined with input_features and output_features\n",
        "Loss function is categorical_crossentropy\n",
        "Adam optimizer is used for the model\n",
        "'''\n",
        "\n",
        "def DNN():\n",
        "    train_input = Input(shape=(vector_size * 2,), name='Inputlayer')\n",
        "    train_in = Dense(512, activation='relu')(train_input)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(256, activation='relu')(train_in)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(event_num)(train_in)\n",
        "    out = Activation('softmax')(train_in)\n",
        "    model = Model(inputs=train_input, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gQ2eyBBRibCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Functions are defined below to evaluate each classifier using various metrics\n",
        "Results are saved as arrays and returned as outputs\n",
        "We can calculate accuracy, roc_aupr, roc_auc, f1_score, precision, recall\n",
        "These metrics are organized by each classifier for easy comparison\n",
        "'''\n",
        "def evaluate(pred_type, pred_score, y_test, event_num, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    # y_one_hot = label_binarize(y_test, np.arange(event_num))\n",
        "    # pred_one_hot = label_binarize(pred_type, np.arange(event_num))\n",
        "    y_one_hot = label_binarize(y_test, classes=np.arange(event_num))\n",
        "    pred_one_hot = label_binarize(pred_type, classes=np.arange(event_num))\n",
        "\n",
        "    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n",
        "\n",
        "    result_all[0] = accuracy_score(y_test, pred_type)\n",
        "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[5] = f1_score(y_test, pred_type, average='micro')\n",
        "    result_all[6] = f1_score(y_test, pred_type, average='macro')\n",
        "    result_all[7] = precision_score(y_test, pred_type, average='micro', zero_division=0)\n",
        "    result_all[8] = precision_score(y_test, pred_type, average='macro', zero_division=0)\n",
        "    result_all[9] = recall_score(y_test, pred_type, average='micro')\n",
        "    result_all[10] = recall_score(y_test, pred_type, average='macro')\n",
        "    for i in range(event_num):\n",
        "        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n",
        "        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                          average=None)\n",
        "        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                         average=None)\n",
        "        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                    average='binary')\n",
        "        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                           average='binary', zero_division=0)\n",
        "        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                        average='binary')\n",
        "    return [result_all, result_eve]\n",
        "\n",
        "\n",
        "def self_metric_calculate(y_true, pred_type):\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred = pred_type.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_pred_c = y_pred.take([0], axis=1).ravel()\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    for i in range(len(y_true_c)):\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 1):\n",
        "            TP += 1\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 0):\n",
        "            FN += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 1):\n",
        "            FP += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 0):\n",
        "            TN += 1\n",
        "    print(\"TP=\", TP, \"FN=\", FN, \"FP=\", FP, \"TN=\", TN)\n",
        "    return (TP / (TP + FP), TP / (TP + FN))\n",
        "\n",
        "\n",
        "def multiclass_precision_recall_curve(y_true, y_score):\n",
        "    y_true = y_true.ravel()\n",
        "    y_score = y_score.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_score.ndim == 1:\n",
        "        y_score = y_score.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_score_c = y_score.take([0], axis=1).ravel()\n",
        "    precision, recall, pr_thresholds = precision_recall_curve(y_true_c, y_score_c)\n",
        "    return (precision, recall, pr_thresholds)\n",
        "\n",
        "\n",
        "def roc_aupr_score(y_true, y_score, average=\"macro\"):\n",
        "    def _binary_roc_aupr_score(y_true, y_score):\n",
        "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _average_binary_score(binary_metric, y_true, y_score, average):  # y_true= y_one_hot\n",
        "        if average == \"binary\":\n",
        "            return binary_metric(y_true, y_score)\n",
        "        if average == \"micro\":\n",
        "            y_true = y_true.ravel()\n",
        "            y_score = y_score.ravel()\n",
        "        if y_true.ndim == 1:\n",
        "            y_true = y_true.reshape((-1, 1))\n",
        "        if y_score.ndim == 1:\n",
        "            y_score = y_score.reshape((-1, 1))\n",
        "        n_classes = y_score.shape[1]\n",
        "        score = np.zeros((n_classes,))\n",
        "        for c in range(n_classes):\n",
        "            y_true_c = y_true.take([c], axis=1).ravel()\n",
        "            y_score_c = y_score.take([c], axis=1).ravel()\n",
        "            score[c] = binary_metric(y_true_c, y_score_c)\n",
        "        return np.average(score)\n",
        "\n",
        "    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)"
      ],
      "metadata": {
        "id": "sw4yXxXsrLg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Training loop as well as cross validation is defined in below function\n",
        "Based on what type of classifier we're using, corresponding model will be used for training\n",
        "For DDIMDL, number of epochs and batch_size can be specified\n",
        "'''\n",
        "number_epochs = 1\n",
        "\n",
        "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    y_score = np.zeros((0, event_num), dtype=float)\n",
        "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
        "    matrix = []\n",
        "    if type(feature_matrix) != list:\n",
        "        matrix.append(feature_matrix)\n",
        "        feature_matrix = matrix\n",
        "    for k in range(CV):\n",
        "        train_index = np.where(index_all_class != k)\n",
        "        test_index = np.where(index_all_class == k)\n",
        "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
        "        for i in range(len(feature_matrix)):\n",
        "            x_train = feature_matrix[i][train_index]\n",
        "            x_test = feature_matrix[i][test_index]\n",
        "            y_train = label_matrix[train_index]\n",
        "            # one-hot encoding\n",
        "            y_train_one_hot = np.array(y_train)\n",
        "            y_train_one_hot = (np.arange(y_train_one_hot.max() + 1) == y_train[:, None]).astype(dtype='float32')\n",
        "            y_test = label_matrix[test_index]\n",
        "            # one-hot encoding\n",
        "            y_test_one_hot = np.array(y_test)\n",
        "            y_test_one_hot = (np.arange(y_test_one_hot.max() + 1) == y_test[:, None]).astype(dtype='float32')\n",
        "            if clf_type == 'DDIMDL':\n",
        "                dnn = DNN()\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "                dnn.fit(x_train, y_train_one_hot, batch_size=128, epochs=number_epochs, validation_data=(x_test, y_test_one_hot),\n",
        "                        callbacks=[early_stopping])\n",
        "                pred += dnn.predict(x_test)\n",
        "                continue\n",
        "            elif clf_type == 'RF':\n",
        "                clf = RandomForestClassifier(n_estimators=100)\n",
        "            elif clf_type == 'GBDT':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'SVM':\n",
        "                clf = SVC(probability=True)\n",
        "            elif clf_type == 'FM':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'KNN':\n",
        "                clf = KNeighborsClassifier(n_neighbors=4)\n",
        "            else:\n",
        "                clf = LogisticRegression(max_iter=1000)\n",
        "            clf.fit(x_train, y_train)\n",
        "            pred += clf.predict_proba(x_test)\n",
        "        pred_score = pred / len(feature_matrix)\n",
        "        pred_type = np.argmax(pred_score, axis=1)\n",
        "        y_true = np.hstack((y_true, y_test))\n",
        "        y_pred = np.hstack((y_pred, pred_type))\n",
        "        y_score = np.row_stack((y_score, pred_score))\n",
        "    result_all, result_eve = evaluate(y_pred, y_score, y_true, event_num, set_name)\n",
        "    return result_all, result_eve"
      ],
      "metadata": {
        "id": "6qi9zZ_Pqw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "*   Hyperparams\n",
        "  *    input layer shape: 572 * 2 = 1144\n",
        "  *    batch_size: 128\n",
        "  *    drop rate: 0.3\n",
        "  *    optimizer: Adam\n",
        "*   Computational Requirements:\n",
        "  *    Hardware: Google Colab CPU runtime\n",
        "  *    Avg runtime of epoch: 6 secs\n",
        "  *    Total training time (DDIMDL): ~40 minutes\n",
        "  *    number of epochs: 100 with EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "\n"
      ],
      "metadata": {
        "id": "O-LOMcPuPygz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Using all the data processing, feature vector, model training functions defined, we can train the model\n",
        "Droprate, vector_size, seed and number of CV folds can be customized\n",
        "Classifiers to be used for comparison can be specified with clf_list\n",
        "\n",
        "'''\n",
        "event_num = 65\n",
        "droprate = 0.3\n",
        "vector_size = 572\n",
        "seed = 0\n",
        "CV = 5\n",
        "interaction_num = 10\n",
        "feature_list = [\"smile\",\"target\",\"enzyme\"] #this is default feature list\n",
        "featureName=\"+\".join(feature_list)\n",
        "\n",
        "# clf_list = [\"DDIMDL\",\"RF\",\"KNN\"]\n",
        "clf_list = [\"DDIMDL\"]\n",
        "for feature in feature_list:\n",
        "    set_name = feature + '+'\n",
        "set_name = set_name[:-1]\n",
        "result_all = {}\n",
        "result_eve = {}\n",
        "all_matrix = []\n",
        "drugList=[]\n",
        "for line in open(\"/content/DL4H_Team_51/DrugList.txt\",'r'):\n",
        "    drugList.append(line.split()[0])\n",
        "\n",
        "extraction = pd.read_sql('select * from extraction;', conn)\n",
        "mechanism = extraction['mechanism']\n",
        "action = extraction['action']\n",
        "drugA = extraction['drugA']\n",
        "drugB = extraction['drugB']\n",
        "\n",
        "for feature in feature_list:\n",
        "    print(feature)\n",
        "    new_feature, new_label, event_num = prepare(df_drug, [feature], vector_size, mechanism,action,drugA,drugB)\n",
        "    all_matrix.append(new_feature)\n",
        "\n",
        "\n",
        "for clf in clf_list:\n",
        "    print(clf)\n",
        "    all_result, each_result = cross_validation(all_matrix, new_label, clf, event_num, seed, CV,\n",
        "                                                set_name)\n",
        "    result_all[clf] = all_result\n",
        "    result_eve[clf] = each_result\n",
        "\n",
        "# with open(f'{model_save_path}/result_ablation.pkl', 'wb') as file:\n",
        "#     pickle.dump(result_all, file)\n",
        "# with open(f'{model_save_path}/result_eve.pkl', 'wb') as file:\n",
        "#     pickle.dump(result_eve, file)\n"
      ],
      "metadata": {
        "id": "-JXAB8U4I3F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading trained model results\n",
        "import pickle\n",
        "\n",
        "with open('/content/DL4H_Team_51/result_all.pkl', 'rb') as file:\n",
        "    result_all = pickle.load(file)\n",
        "with open('/content/DL4H_Team_51/result_eve.pkl', 'rb') as file:\n",
        "    result_eve = pickle.load(file)"
      ],
      "metadata": {
        "id": "mJzGnhzBGKdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Study\n",
        "\n",
        "For our ablation study, we removed the first Dropout layer from the model to see how the results will differ from the original model. To do this, we commented out the first Dropout layer and only included DDIMDL in the clf_list. The results of both the ablation study and the original model for DDIMDL is saved in the 'result_ablation' file which we will use to plot our graphs and compare the results. Our prediction was that the modified model will perform poorer than the original since dropout layers are used to prevent overfitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GwAd7TrCJ8aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/DL4H_Team_51/result_ablation.pkl', 'rb') as file:\n",
        "    result_ablation = pickle.load(file)"
      ],
      "metadata": {
        "id": "Lr70Mw3PXkXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to generate chart with metrics for model comparison with other standard models\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_overall_evaluation_results(result_all):\n",
        "    # Overall Evaluation Metrics\n",
        "    overall_metrics_labels = [\n",
        "        \"Accuracy\",\n",
        "        \"ROC AUPR (micro)\",\n",
        "        \"ROC AUPR (macro)\",\n",
        "        \"ROC AUC (micro)\",\n",
        "        \"ROC AUC (macro)\",\n",
        "        \"F1 Score (micro)\",\n",
        "        \"F1 Score (macro)\",\n",
        "        \"Precision (micro)\",\n",
        "        \"Precision (macro)\",\n",
        "        \"Recall (micro)\",\n",
        "        \"Recall (macro)\"\n",
        "    ]\n",
        "\n",
        "    n_classifiers = len(result_all)\n",
        "    bar_width = 0.25\n",
        "\n",
        "    indices = np.arange(len(overall_metrics_labels))\n",
        "    positions = [indices + i * bar_width for i in range(n_classifiers)]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for idx, (classifier_name, results) in enumerate(result_all.items()):\n",
        "        overall_metrics_values = [val[0] for val in results]\n",
        "        plt.bar(positions[idx], overall_metrics_values, width=bar_width, label=classifier_name)\n",
        "\n",
        "    plt.xlabel('Score', fontsize=12)\n",
        "    plt.ylabel('Metrics', fontsize=12)\n",
        "    plt.title('Comparison of Overall Evaluation Metrics by Classifier', fontsize=16)\n",
        "    plt.xticks([r + bar_width for r in range(len(overall_metrics_labels))], overall_metrics_labels, rotation=30)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_overall_evaluation_results(result_ablation)"
      ],
      "metadata": {
        "id": "R0f72vAEMDn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comparison_table(result_all):\n",
        "    metrics = [\"Accuracy\", \"ROC AUPR (micro)\", \"ROC AUPR (macro)\", \"ROC AUC (micro)\",\n",
        "               \"ROC AUC (macro)\", \"F1 Score (micro)\", \"F1 Score (macro)\",\n",
        "               \"Precision (micro)\", \"Precision (macro)\", \"Recall (micro)\", \"Recall (macro)\"]\n",
        "\n",
        "    data = {classifier_name: [result[0] for result in results] for classifier_name, results in result_all.items()}\n",
        "\n",
        "    df = pd.DataFrame(data, index=metrics)\n",
        "\n",
        "    return df\n",
        "\n",
        "comparison_table = create_comparison_table(result_ablation)\n",
        "print(comparison_table)"
      ],
      "metadata": {
        "id": "j5vVVSt0MNhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of this ablation study confirmed our predictions as all but one of the evaluation metrics was higher in the original model compared to the modified model."
      ],
      "metadata": {
        "id": "53q85sKTZBRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "•\tAccuracy: Measures the percentage of correct predictions for all classes\n",
        "\n",
        "•\tROC AUPR (micro): The area under precision-recall curve for all classes\n",
        "\n",
        "•\tROC AUPR (macro): The area under precision-recall curve for for each class and averaged\n",
        "\n",
        "•\tROC AUC (micro): The area under the receiver operating characteristic curve, aggregating outcomes of all classes to evaluate the overall performance\n",
        "\n",
        "•\tROC AUC (macro): Average area under the curve for each class, calculated separately and then averaged, giving equal weight to each class\n",
        "\n",
        "•\tF1 Score (micro): Harmonic mean of precision and recall for all classes\n",
        "\n",
        "•\tF1 Score (macro): Calculates the F1 score for each class individually and averages them\n",
        "\n",
        "•\tPrecision (micro): Proportion of correct positive predictions across all classes\n",
        "\n",
        "•\tPrecision (macro): Mean of precision scores for each class, calculated separately\n",
        "\n"
      ],
      "metadata": {
        "id": "JfMcclntd9Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to generate chart with metrics for model comparison with other standard models\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_overall_evaluation_results(result_all):\n",
        "    # Overall Evaluation Metrics\n",
        "    overall_metrics_labels = [\n",
        "        \"Accuracy\",\n",
        "        \"ROC AUPR (micro)\",\n",
        "        \"ROC AUPR (macro)\",\n",
        "        \"ROC AUC (micro)\",\n",
        "        \"ROC AUC (macro)\",\n",
        "        \"F1 Score (micro)\",\n",
        "        \"F1 Score (macro)\",\n",
        "        \"Precision (micro)\",\n",
        "        \"Precision (macro)\",\n",
        "        \"Recall (micro)\",\n",
        "        \"Recall (macro)\"\n",
        "    ]\n",
        "\n",
        "    n_classifiers = len(result_all)\n",
        "    bar_width = 0.25\n",
        "\n",
        "    indices = np.arange(len(overall_metrics_labels))\n",
        "    positions = [indices + i * bar_width for i in range(n_classifiers)]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for idx, (classifier_name, results) in enumerate(result_all.items()):\n",
        "        overall_metrics_values = [val[0] for val in results]\n",
        "        plt.bar(positions[idx], overall_metrics_values, width=bar_width, label=classifier_name)\n",
        "\n",
        "    plt.xlabel('Score', fontsize=12)\n",
        "    plt.ylabel('Metrics', fontsize=12)\n",
        "    plt.title('Comparison of Overall Evaluation Metrics by Classifier', fontsize=16)\n",
        "    plt.xticks([r + bar_width for r in range(len(overall_metrics_labels))], overall_metrics_labels, rotation=30)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_overall_evaluation_results(result_all)"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comparison_table(result_all):\n",
        "    metrics = [\"Accuracy\", \"ROC AUPR (micro)\", \"ROC AUPR (macro)\", \"ROC AUC (micro)\",\n",
        "               \"ROC AUC (macro)\", \"F1 Score (micro)\", \"F1 Score (macro)\",\n",
        "               \"Precision (micro)\", \"Precision (macro)\", \"Recall (micro)\", \"Recall (macro)\"]\n",
        "\n",
        "    data = {classifier_name: [result[0] for result in results] for classifier_name, results in result_all.items()}\n",
        "\n",
        "    df = pd.DataFrame(data, index=metrics)\n",
        "\n",
        "    return df\n",
        "\n",
        "comparison_table = create_comparison_table(result_all)\n",
        "print(comparison_table)"
      ],
      "metadata": {
        "id": "zmR1B3LdhLcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "*   With the notebook, we were able to achieve very similar scores for all metrics for the DDIMDL model\n",
        "*   As hypothesized, DDIMDL model performed better than other standard classifiers for all metrics. Biggest differences can be seen from F1 Score (macro) and Recall (macro)\n",
        "*   Ablation study was performed with one layer removed and it performed worse than the original setup of the DDIMDL model. This was also stated by the paper and proven with results.\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "*   The paper is fully reproducible in Google Colab Notebook environment\n",
        "*   What was easy\n",
        "  *    Working with clearly defined sections/functions for fitting into Colab Notebook\n",
        "  *    Not too complex code for feature engineering, training, cross-validation\n",
        "  *    Easy to test other types of models for comparison using built in functions\n",
        "*    What was difficult\n",
        "  *    Figuring out what packages/functions are deprecated with Colab packages\n",
        "  *    Modernizing inputs and outputs of functions that require different syntax\n",
        "  *    Rewriting functions to have them work with one notebook instead of working with several different script files\n",
        "*   Suggestions to the author\n",
        "  *    Use modern packages and rewrite couple functions to make them work with modern packages\n",
        "  *    Clearer instructions for initializing the model with user inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8SI64iYv_Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   @article{deng2020multimodal,\n",
        "title={A multimodal deep learning framework for predicting drug-drug interaction events},\n",
        "author={Deng, Yifan and Xu, Xinran and Qiu, Yang and Xia, Jingbo and Zhang, Wen and Liu, Shichao},\n",
        "journal={Bioinformatics}\n",
        "}\n",
        "\n",
        "2. https://github.com/YifanDengWHU/DDIMDL\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}